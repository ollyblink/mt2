\chapter{Related Work and State of the Art}
Although the widest usage of the MapReduce programming model to date may be the one of Hadoop \cite{Hadoop} with its centralised master-slave architecture, recently, there have also been a number of attempts to transport it to a more decentralised setting in an endeavour to support currently popular Cloud platforms, pervasive grids, and/or mobile environments. Such endeavours address the problem of Hadoop (and similar MapReduce implementations) being designed for dedicate environments and not supporting dynamic environments with high churn rates \cite{CReSTIC-2677}.

\cite{Hadoop} is an open-source implementation of MapReduce (mainly implemented by Yahoo) enjoying a wide adoption and is often used for short jobs, where low response time is critical \cite{Zaharia:2008:IMP:1855741.1855744}. It's implementation closely resembles Google's \cite{Dean2008}, where master nodes manage a number of slave nodes. The input file resides in the distributed file system (HDFS \cite{Lin:2010:MMO:1851476.1851489}) and is split in even chunks replicated for fault-tolerance \cite{Zaharia:2008:IMP:1855741.1855744}. Output of map tasks are not replicated. \cite{Lin:2010:MMO:1851476.1851489}. task scheduler implicitly assumes cluster nodes to be homogeneous and tasks progress linearly to decide if a task should be re-executed (in homogeneous environments, execution should be more or less equal on each node). This makes Hadoop rather unrealistic for "office" settings, where commodity hardware may vary greatly in performance. 

\cite{Marozzo2012a} recognised the problem of centralised master-slave architectures to not cope well with dynamic Cloud infrastructures, where nodes may join or leave the network at high rates. Thus, they introduce a peer-to-peer model to manage node churn but also master failures and job recovery in a decentralised way. Each node may become a master or a slave at any given time dynamically, preserving a certain master/slave ratio. Slaves are assigned tasks to perform by the masters, which handle management, recovery, and coordination. To reduce job loss in case of master failures, each master may act as a backup master for a certain job, only executing it if the master primarily responsible for that job fails. Overall, the structure of different entities resembles very much the one of Hadoop, simply ported to a P2P setting. They were able to show that such an implementation provides better fault tolerance levels compared to a centralised implementations of MapReduce and only limited impact on network overhead. 

In a very similar direction goes the idea of CloudFIT \cite{Steffenel2015}, which the authors advertise as a "Platform-as-a-Service (PaaS) middleware that allows the creation of private clouds over pervasive environments". The idea is to use private computers to set up a private "Cloud", and that MapReduce jobs are then distributed to this environment and executed in a P2P fashion. CloudFIT is, thus, built to support various P2P overlays and the authors show the performance of CloudFIT with both PAST and TomP2P against Hadoop. The results demonstrated CloudFIT to be able to achieve similar execution speeds as Hadoop while omitting the need of a dedicated cluster of computers. Data in CloudFIT is directly stored within the DHT of the corresponding overlay, such that it is replicated by a factor of k. Although such an implementation may not guarantee n-resiliency (meaning, replicating the data on each node), it is a reasonable assumption that fault tolerance may be ensured more than enough (provided at least a number of nodes stay alive and the complete dataset is replicated on these nodes) while drastically improving storage performance. Furthermore, not all the data is broadcasted but only the keys of each task, further reducing the network workload the authors showed in another study \cite{Steffenel2015a} to be one of the main problems in distributed environments for the performance of MapReduce tasks.

